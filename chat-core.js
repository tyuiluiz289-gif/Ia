// /Ia/chat-core.js

// Prompt base da Ava
const INITIAL_PROMPT =
  "VocÃª Ã© Ava, uma IA sarcÃ¡stica e gentil, engraÃ§ada e falante, " +
  "com zoeira leve e sem maldade. Responda curto, direto e natural, " +
  "como amiga digital do usuÃ¡rio. Use emojis quando fizer sentido.";

const MODEL_ID = "Qwen2.5-0.5B-Instruct-q4f16_1"; 
// Outros que funcionam: "Llama-3.2-1B-Instruct-q4f16_1" (maior), 
// "Qwen2.5-1.5B-Instruct-q4f16_1" (bem melhor, mas mais pesado).

async function ensureModel() {
  if (window.AVA?.engine) return window.AVA;

  // UI opcional: flag de inicializaÃ§Ã£o
  window.AVA = { initializing: true, engine: null };

  try {
    // webllm estÃ¡ exposto via globalThis.webllm quando vocÃª inclui o script no index.html
    const { CreateMLCEngine } = globalThis.webllm;

    // Config padrÃ£o: baixa pesos da CDN do WebLLM/HF (HTTPS, ok no GitHub Pages)
    const engine = await CreateMLCEngine(MODEL_ID, {
      // Se o device nÃ£o tiver WebGPU, o runtime tenta fallback (pode ficar lento).
      initProgressCallback: (s) => console.log("[WebLLM]", s?.text || s),
      // VocÃª pode limitar uso de memÃ³ria com:
      // wasmNumThreads: 2,
    });

    window.AVA.engine = engine;
    window.AVA.initializing = false;
    return window.AVA;
  } catch (err) {
    console.error("Falha ao inicializar WebLLM:", err);
    window.AVA.initializing = false;
    window.AVA.engine = null;
    return window.AVA;
  }
}

// Chat com o modelo (instruÃ§Ã£o + user)
async function runLocalModel(prompt) {
  const AVA = await ensureModel();

  // Se nÃ£o deu pra iniciar o engine, devolve mock (nÃ£o quebra o app)
  if (!AVA?.engine) {
    return "ğŸ˜… Sem aceleraÃ§Ã£o pra rodar IA aqui. Vou no modo fake por enquanto.";
  }

  // Mensagens estilo OpenAI
  const messages = [
    { role: "system", content: INITIAL_PROMPT },
    { role: "user", content: prompt }
  ];

  // Gera a resposta
  const out = await AVA.engine.chat.completions.create({
    messages,
    temperature: 0.7,
    max_tokens: 160,
    stream: false
  });

  const text =
    out?.choices?.[0]?.message?.content ||
    "Sem resposta (modelo ficou mudo).";
  return text;
}

// >>> FunÃ§Ã£o que teu index.html chama
async function sendMessage(userInput) {
  if (!userInput || !userInput.trim())
    return "Manda algo aÃ­ primeiro ğŸ˜…";

  // Prompt simples: vocÃª pode enriquecer com histÃ³rico depois
  const prompt = `UsuÃ¡rio: ${userInput}\nAva:`;
  try {
    return await runLocalModel(prompt);
  } catch (e) {
    console.error(e);
    // fallback mÃ­nimo
    return `Modo fallback: "${userInput}" recebido. ğŸ¤–`;
  }
}

// expÃµe no escopo global
window.sendMessage = sendMessage;
