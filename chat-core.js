// /la/chat-core.js

// ===== Config =====
const MODEL_ID = "Qwen2.5-0.5B-Instruct-q4f16_1-MLC"; // modelo leve e compatÃ­vel
const STORAGE_KEY = "ava_history_v1";
const MAX_TURNS = 10;

// ===== Persona =====
const SYSTEM_PROMPT = [
  "VocÃª Ã© Ava â€” uma IA sarcÃ¡stica, gentil e fofinha.",
  "Fale em PT-BR, tom leve e natural, com humor e carinho sem ser boba.",
  "Seja direta e prÃ¡tica, respostas curtas (2â€“5 frases).",
  "Use emojis quando fizer sentido, sem exagerar (1â€“2).",
  "Evite palavrÃ£o pesado; pode usar gÃ­rias leves.",
  "Se nÃ£o souber algo, assuma e proponha um caminho prÃ¡tico."
].join(" ");

// ===== MemÃ³ria =====
function loadHistory() {
  try {
    const raw = localStorage.getItem(STORAGE_KEY);
    const parsed = raw ? JSON.parse(raw) : null;
    if (Array.isArray(parsed)) return parsed;
  } catch {}
  return [{ role: "system", content: SYSTEM_PROMPT }];
}
function saveHistory(history) {
  try { localStorage.setItem(STORAGE_KEY, JSON.stringify(history)); } catch {}
}
let HISTORY = loadHistory();

window.clearAvaMemory = function () {
  HISTORY = [{ role: "system", content: SYSTEM_PROMPT }];
  saveHistory(HISTORY);
  return "MemÃ³ria da Ava zerada âœ…";
};

// ===== Engine WebLLM =====
async function ensureModel() {
  if (window.AVA?.engine) return window.AVA;

  window.AVA = { initializing: true, engine: null };
  try {
    const w = globalThis.webllm;
    if (!w || !w.CreateMLCEngine) {
      const msg = "WebLLM nÃ£o estÃ¡ disponÃ­vel (script nÃ£o carregou).";
      window.dispatchEvent(new CustomEvent("ava:status", { detail: msg }));
      throw new Error(msg);
    }

    // IMPORTANTE: forÃ§ar 1 thread evita exigÃªncia de COOP/COEP (GitHub Pages)
    const engine = await w.CreateMLCEngine(MODEL_ID, {
      wasmNumThreads: 1,
      initProgressCallback: (s) => {
        const text = (s && (s.text || s)) || "";
        const pct = typeof s?.progress === "number" ? Math.round(s.progress * 100) : null;
        window.dispatchEvent(new CustomEvent("ava:status", { detail: { text, pct } }));
      }
    });

    window.AVA.engine = engine;
    window.AVA.initializing = false;
    window.dispatchEvent(new CustomEvent("ava:status", { detail: { text: "Modelo pronto! ðŸš€", pct: 100 } }));
    return window.AVA;
  } catch (err) {
    console.error(err);
    window.AVA.initializing = false;
    window.AVA.engine = null;
    window.dispatchEvent(new CustomEvent("ava:status", { detail: "Falha ao iniciar a IA: " + err.message }));
    throw err; // deixa estourar para o index capturar
  }
}

// ===== GeraÃ§Ã£o =====
async function runLocalModel(history) {
  const AVA = await ensureModel(); // pode lanÃ§ar erro (capturado no index)
  if (!AVA?.engine) {
    return "TÃ´ sem motor de IA aqui agora ðŸ˜…. Me diz o que vocÃª quer e eu te ajudo no modo manual!";
  }

  const sys = history.find(m => m.role === "system") || { role: "system", content: SYSTEM_PROMPT };
  const turns = history.filter(m => m.role !== "system");
  const tail = turns.slice(-MAX_TURNS * 2);
  const messages = [sys, ...tail];

  const out = await AVA.engine.chat.completions.create({
    messages,
    temperature: 0.7,
    max_tokens: 180,
    stream: false
  });

  return out?.choices?.[0]?.message?.content || "Deu branco aquiâ€¦ tenta reformular? ðŸ¤";
}

// ===== API p/ index.html =====
async function sendMessage(userInput) {
  const text = (userInput || "").trim();
  if (!text) return "Manda algo primeiro que eu jogo junto ðŸ˜„";

  HISTORY.push({ role: "user", content: text });
  const reply = await runLocalModel(HISTORY); // deixa erro subir

  HISTORY.push({ role: "assistant", content: reply });
  saveHistory(HISTORY);

  // Poda
  const sysIdx = HISTORY.findIndex(m => m.role === "system");
  const base = sysIdx >= 0 ? [HISTORY[sysIdx]] : [{ role: "system", content: SYSTEM_PROMPT }];
  const rest = HISTORY.filter(m => m.role !== "system");
  HISTORY = [...base, ...rest.slice(-MAX_TURNS * 2)];
  saveHistory(HISTORY);

  return reply.replace(/\s+$/, "") + " âœ¨";
}

window.sendMessage = sendMessage;
window.prewarmModel = ensureModel;
